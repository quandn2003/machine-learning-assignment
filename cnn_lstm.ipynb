{"cells":[{"cell_type":"code","execution_count":445,"metadata":{"id":"8TFX7wSAmg9y"},"outputs":[],"source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","from string import digits\n","from collections import Counter\n","from pyvi import ViTokenizer\n","from gensim.models.word2vec import Word2Vec\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":446,"metadata":{"id":"a7lMy03omg93","scrolled":true},"outputs":[],"source":["data_train = pd.read_csv(\"vlsp_sentiment_train.csv\", sep='\\t')\n","data_train.columns =['Class', 'Data']\n","data_test = pd.read_csv(\"vlsp_sentiment_test.csv\", sep='\\t')\n","data_test.columns =['Class', 'Data']"]},{"cell_type":"code","execution_count":447,"metadata":{"id":"4HR1jAzImg94"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>Data</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1</td>\n","      <td>Mình đã dùng anywhere thế hệ đầu, quả là đầy t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1</td>\n","      <td>Quan tâm nhất là độ trễ có cao không, dùng thi...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1</td>\n","      <td>dag xài con cùi bắp 98k....pin trâu, mỗi tội đ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-1</td>\n","      <td>logitech chắc hàng phải tiền triệu trở lên dùn...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-1</td>\n","      <td>Đang xài con m175 cùi mía , nhà xài nhiều chuộ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Class                                               Data\n","0     -1  Mình đã dùng anywhere thế hệ đầu, quả là đầy t...\n","1     -1  Quan tâm nhất là độ trễ có cao không, dùng thi...\n","2     -1  dag xài con cùi bắp 98k....pin trâu, mỗi tội đ...\n","3     -1  logitech chắc hàng phải tiền triệu trở lên dùn...\n","4     -1  Đang xài con m175 cùi mía , nhà xài nhiều chuộ..."]},"execution_count":447,"metadata":{},"output_type":"execute_result"}],"source":["data_train.head()"]},{"cell_type":"code","execution_count":448,"metadata":{},"outputs":[],"source":["data_train = data_train.sample(frac=1, random_state=42)"]},{"cell_type":"code","execution_count":449,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>Data</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4039</th>\n","      <td>0</td>\n","      <td>tuy có sự sáng tạo , nhưng cần phải có phong c...</td>\n","    </tr>\n","    <tr>\n","      <th>3815</th>\n","      <td>0</td>\n","      <td>khoảng 3-4s j đó</td>\n","    </tr>\n","    <tr>\n","      <th>848</th>\n","      <td>-1</td>\n","      <td>Chiều dài 45cm :( bỏ vào túi kiểu gì</td>\n","    </tr>\n","    <tr>\n","      <th>4863</th>\n","      <td>0</td>\n","      <td>không , không nên mua . mua samsung ngon hơn .</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>-1</td>\n","      <td>thế thì quất thôi, chứ con miband 1s của e bên...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Class                                               Data\n","4039      0  tuy có sự sáng tạo , nhưng cần phải có phong c...\n","3815      0                                   khoảng 3-4s j đó\n","848      -1               Chiều dài 45cm :( bỏ vào túi kiểu gì\n","4863      0     không , không nên mua . mua samsung ngon hơn .\n","79       -1  thế thì quất thôi, chứ con miband 1s của e bên..."]},"execution_count":449,"metadata":{},"output_type":"execute_result"}],"source":["data_train.head()"]},{"cell_type":"code","execution_count":450,"metadata":{"id":"jvrbwPfZmg95"},"outputs":[],"source":["labels = data_train.iloc[:, 0].values\n","reviews = data_train.iloc[:, 1].values"]},{"cell_type":"code","execution_count":451,"metadata":{"id":"3HlbVeHimg95"},"outputs":[],"source":["encoded_labels = []\n","\n","for label in labels:\n","    if label == -1:\n","        encoded_labels.append([1,0,0])\n","    elif label == 0:\n","        encoded_labels.append([0,1,0])\n","    else:\n","        encoded_labels.append([0,0,1])\n","\n","encoded_labels = np.array(encoded_labels)"]},{"cell_type":"code","execution_count":452,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 1 0]\n","[0 1 0]\n","[1 0 0]\n","[0 1 0]\n","[1 0 0]\n"]}],"source":["for i in range(5):\n","    print(encoded_labels[i])"]},{"cell_type":"code","execution_count":453,"metadata":{"id":"Lm4OCwxXmg96"},"outputs":[],"source":["reviews_processed = []\n","unlabeled_processed = []\n","for review in reviews:\n","    review_cool_one = ''.join([char for char in review if char not in digits])\n","    reviews_processed.append(review_cool_one)"]},{"cell_type":"code","execution_count":454,"metadata":{"id":"nW2OZgkgmg97"},"outputs":[],"source":["#Use PyVi for Vietnamese word tokenizer\n","word_reviews = []\n","all_words = []\n","for review in reviews_processed:\n","    review = ViTokenizer.tokenize(review.lower())\n","    word_reviews.append(review.split())\n"]},{"cell_type":"code","execution_count":455,"metadata":{"id":"t-8cJ7z4z2C0"},"outputs":[{"data":{"text/plain":["['tuy',\n"," 'có',\n"," 'sự',\n"," 'sáng_tạo',\n"," ',',\n"," 'nhưng',\n"," 'cần',\n"," 'phải',\n"," 'có',\n"," 'phong_cách',\n"," 'riêng',\n"," ',',\n"," 'đừng',\n"," 'chạy',\n"," 'theo',\n"," 'iphone',\n"," ',',\n"," 'samsung',\n"," 'rất',\n"," 'cố_gắng',\n"," ',',\n"," 'biết',\n"," 'nắm_bắt',\n"," 'nhu_cầu',\n"," 'khách_hàng',\n"," '(',\n"," 's',\n"," 's',\n"," 'edge',\n"," 'người',\n"," 'châu',\n"," 'á',\n"," 'rất',\n"," 'chuộng',\n"," ',',\n"," 'nhưng',\n"," 'ko',\n"," 'thấy',\n"," 'phát_triển',\n"," 'nữa',\n"," ')',\n"," 's',\n"," 'là',\n"," 'sự',\n"," 'hoàn_thiện',\n"," 'của',\n"," 's',\n"," ',',\n"," 'nhưng',\n"," 'sfan',\n"," 'thì',\n"," 'luôn',\n"," 'gato',\n"," ',',\n"," 'vì',\n"," 'các',\n"," 'bạn',\n"," 'ấy',\n"," 'thấy',\n"," 'iphone',\n"," 'quá',\n"," 'đắt',\n"," 'và',\n"," 'các',\n"," 'bạn',\n"," 'ấy',\n"," 'chuẩn_bị',\n"," 'lên_tiếng',\n"," '.']"]},"execution_count":455,"metadata":{},"output_type":"execute_result"}],"source":["word_reviews[0]"]},{"cell_type":"code","execution_count":456,"metadata":{"id":"pTb0MeDRmg98"},"outputs":[],"source":["EMBEDDING_DIM = 400 # how big is each word vector\n","MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n","MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"]},{"cell_type":"code","execution_count":457,"metadata":{"id":"jW-7mKtWmg9-"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":458,"metadata":{"id":"-BHpPSLTmg9_"},"outputs":[],"source":["tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n","tokenizer.fit_on_texts(word_reviews)\n","sequences_train = tokenizer.texts_to_sequences(word_reviews)\n","word_index = tokenizer.word_index\n"]},{"cell_type":"code","execution_count":459,"metadata":{"id":"fX8asUU4MxMK"},"outputs":[],"source":["# word_index[1]"]},{"cell_type":"code","execution_count":460,"metadata":{"id":"LlV3M2dimg9_"},"outputs":[],"source":["data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n","labels = encoded_labels"]},{"cell_type":"code","execution_count":461,"metadata":{"id":"PTfd3gXdNoU4"},"outputs":[{"data":{"text/plain":["array([[   0,    0,    0, ...,  950, 2022,    1],\n","       [   0,    0,    0, ...,   60,  309,   62],\n","       [   0,    0,    0, ...,  726,  310,   43],\n","       ...,\n","       [   0,    0,    0, ...,   11,  434, 1036],\n","       [   0,    0,    0, ..., 4142,    6,  158],\n","       [   0,    0,    0, ...,   33,   33,    1]])"]},"execution_count":461,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"code","execution_count":462,"metadata":{"id":"4dl9VZ3Rmg-A"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X train and X validation tensor: (5100, 300)\n","Shape of label train and validation tensor: (5100, 3)\n"]}],"source":["print('Shape of X train and X validation tensor:',data.shape)\n","print('Shape of label train and validation tensor:', labels.shape)"]},{"cell_type":"code","execution_count":463,"metadata":{"id":"-KKSjJdJmg-A"},"outputs":[],"source":["import gensim\n","from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","\n","word_vectors = KeyedVectors.load_word2vec_format('vi-model-CBOW.bin', binary=True)\n","\n","\n","vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n","embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    if i>=MAX_VOCAB_SIZE:\n","        continue\n","    try:\n","        embedding_vector = word_vectors[word]\n","        embedding_matrix[i] = embedding_vector\n","    except KeyError:\n","        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n","\n","del(word_vectors)\n","\n","from keras.layers import Embedding\n","embedding_layer = Embedding(vocabulary_size,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            trainable=True)"]},{"cell_type":"code","execution_count":464,"metadata":{},"outputs":[],"source":["from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, MaxPooling2D, Embedding, BatchNormalization, AveragePooling1D, LSTM, Bidirectional\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input, Dense, Embedding, Dropout,concatenate\n","from tensorflow.keras.layers import Reshape, Flatten\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.optimizers import Adam, SGD\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import regularizers"]},{"cell_type":"code","execution_count":465,"metadata":{"id":"njBANdn5mg-B"},"outputs":[{"name":"stdout","output_type":"stream","text":["<keras.src.engine.functional.Functional object at 0x000002BE60769810>\n","Model: \"model_16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_17 (InputLayer)       [(None, 300)]             0         \n","                                                                 \n"," embedding_15 (Embedding)    (None, 300, 400)          3167600   \n","                                                                 \n"," conv1d_16 (Conv1D)          (None, 297, 64)           102464    \n","                                                                 \n"," max_pooling1d_16 (MaxPooli  (None, 296, 64)           0         \n"," ng1D)                                                           \n","                                                                 \n"," dropout_44 (Dropout)        (None, 296, 64)           0         \n","                                                                 \n"," bidirectional_15 (Bidirect  (None, 128)               66048     \n"," ional)                                                          \n","                                                                 \n"," dropout_45 (Dropout)        (None, 128)               0         \n","                                                                 \n"," dense_16 (Dense)            (None, 3)                 387       \n","                                                                 \n","=================================================================\n","Total params: 3336499 (12.73 MB)\n","Trainable params: 3336499 (12.73 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["sequence_length = data.shape[1]\n","filter_sizes = [3,4,5]\n","num_filters = 100\n","drop = 0.5\n","\n","inputs = Input(shape=(sequence_length,))\n","embedding = embedding_layer(inputs)\n","# dropout1 = Dropout(drop)(embedding)\n","dropout1 = embedding\n","conv = Conv1D(64, 4, activation='relu', kernel_regularizer=regularizers.l2(0.01))(dropout1)\n","maxpool_2 = MaxPooling1D(2, strides=1)(conv)\n","dropout2 = Dropout(drop)(maxpool_2)\n","lstm = Bidirectional(LSTM(64))(dropout2)\n","dropout3 = Dropout(drop)(lstm)\n","output = Dense(units=3, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout3)\n","\n","model = Model(inputs, output)\n","print(model)\n","\n","adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","model.summary()\n","\n","early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=10, verbose=1)"]},{"cell_type":"code","execution_count":466,"metadata":{"colab":{"background_save":true},"id":"Jn0dBlzjmg-D"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","16/16 [==============================] - 24s 1s/step - loss: 2.1097 - accuracy: 0.3703 - val_loss: 1.8727 - val_accuracy: 0.4500\n","Epoch 2/50\n","16/16 [==============================] - 19s 1s/step - loss: 1.7240 - accuracy: 0.4547 - val_loss: 1.5450 - val_accuracy: 0.5225\n","Epoch 3/50\n","16/16 [==============================] - 19s 1s/step - loss: 1.4279 - accuracy: 0.5277 - val_loss: 1.3517 - val_accuracy: 0.5520\n","Epoch 4/50\n","16/16 [==============================] - 18s 1s/step - loss: 1.2426 - accuracy: 0.5752 - val_loss: 1.1587 - val_accuracy: 0.6078\n","Epoch 5/50\n","16/16 [==============================] - 18s 1s/step - loss: 1.0917 - accuracy: 0.6331 - val_loss: 1.1700 - val_accuracy: 0.5667\n","Epoch 6/50\n","16/16 [==============================] - 18s 1s/step - loss: 0.9920 - accuracy: 0.6767 - val_loss: 1.0539 - val_accuracy: 0.6392\n","Epoch 7/50\n","16/16 [==============================] - 18s 1s/step - loss: 0.9012 - accuracy: 0.7211 - val_loss: 1.0334 - val_accuracy: 0.6578\n","Epoch 8/50\n","16/16 [==============================] - 19s 1s/step - loss: 0.8288 - accuracy: 0.7578 - val_loss: 1.0744 - val_accuracy: 0.6284\n","Epoch 9/50\n","16/16 [==============================] - 18s 1s/step - loss: 0.7901 - accuracy: 0.7806 - val_loss: 1.0437 - val_accuracy: 0.6510\n","Epoch 10/50\n","16/16 [==============================] - 18s 1s/step - loss: 0.7069 - accuracy: 0.8211 - val_loss: 1.0746 - val_accuracy: 0.6510\n","Epoch 11/50\n","16/16 [==============================] - 18s 1s/step - loss: 0.6547 - accuracy: 0.8375 - val_loss: 1.1290 - val_accuracy: 0.6441\n","Epoch 12/50\n","16/16 [==============================] - 18s 1s/step - loss: 0.5681 - accuracy: 0.8784 - val_loss: 1.2064 - val_accuracy: 0.6520\n","Epoch 13/50\n","16/16 [==============================] - 18s 1s/step - loss: 0.5418 - accuracy: 0.8963 - val_loss: 1.2272 - val_accuracy: 0.6353\n","Epoch 14/50\n","16/16 [==============================] - 18s 1s/step - loss: 0.5960 - accuracy: 0.8728 - val_loss: 1.2222 - val_accuracy: 0.6520\n","Epoch 15/50\n","16/16 [==============================] - 19s 1s/step - loss: 0.5036 - accuracy: 0.9098 - val_loss: 1.2548 - val_accuracy: 0.6608\n","Epoch 16/50\n","16/16 [==============================] - 18s 1s/step - loss: 0.4741 - accuracy: 0.9248 - val_loss: 1.2671 - val_accuracy: 0.6343\n","Epoch 17/50\n","16/16 [==============================] - 19s 1s/step - loss: 0.4212 - accuracy: 0.9441 - val_loss: 1.3127 - val_accuracy: 0.6657\n","Epoch 17: early stopping\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x2be618c3190>"]},"execution_count":466,"metadata":{},"output_type":"execute_result"}],"source":["checkpoint = ModelCheckpoint('cnn_lstm.keras',\n","                             monitor='val_accuracy',\n","                             save_best_only=True, verbose=False, mode='max')\n","callbacks_list = [checkpoint, early_stopping]\n","\n","model.fit(data, labels, validation_split=0.2,\n","          epochs=50 , batch_size=256, callbacks=callbacks_list, shuffle=True)"]},{"cell_type":"code","execution_count":467,"metadata":{"colab":{"background_save":true},"id":"8XoN2UOamg-D"},"outputs":[],"source":["labels_test = data_test.iloc[:, 0].values\n","reviews_test = data_test.iloc[:, 1].values"]},{"cell_type":"code","execution_count":468,"metadata":{"colab":{"background_save":true},"id":"PwiYb3Ohmg-E"},"outputs":[],"source":["encoded_labels_test = []\n","\n","for label_test in labels_test:\n","    if label_test == -1:\n","        encoded_labels_test.append([1,0,0])\n","    elif label_test == 0:\n","        encoded_labels_test.append([0,1,0])\n","    else:\n","        encoded_labels_test.append([0,0,1])\n","\n","encoded_labels_test = np.array(encoded_labels_test)"]},{"cell_type":"code","execution_count":469,"metadata":{"colab":{"background_save":true},"id":"E08tBw9img-E"},"outputs":[],"source":["reviews_processed_test = []\n","unlabeled_processed_test = []\n","for review_test in reviews_test:\n","    review_cool_one = ''.join([char for char in review_test])\n","    reviews_processed_test.append(review_cool_one)"]},{"cell_type":"code","execution_count":470,"metadata":{"colab":{"background_save":true},"id":"OwgI9Xywmg-E"},"outputs":[],"source":["#Use PyVi for Vietnamese word tokenizer\n","word_reviews_test = []\n","all_words = []\n","for review_test in reviews_processed_test:\n","    review_test = ViTokenizer.tokenize(review_test.lower())\n","    word_reviews_test.append(review_test.split())"]},{"cell_type":"code","execution_count":471,"metadata":{"colab":{"background_save":true},"id":"p02GxCh6mg-F"},"outputs":[],"source":["sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n","data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n","labels_test = encoded_labels_test"]},{"cell_type":"code","execution_count":472,"metadata":{"colab":{"background_save":true},"id":"jAqUMGInmg-F"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X train and X validation tensor: (1050, 300)\n","Shape of label train and validation tensor: (1050, 3)\n"]}],"source":["print('Shape of X train and X validation tensor:',data_test.shape)\n","print('Shape of label train and validation tensor:', labels_test.shape)"]},{"cell_type":"code","execution_count":473,"metadata":{"colab":{"background_save":true},"id":"LKclttiOmg-F"},"outputs":[{"name":"stdout","output_type":"stream","text":["33/33 [==============================] - 2s 46ms/step - loss: 1.2779 - accuracy: 0.6590\n"]}],"source":["# model = load_model('best_model.keras')\n","score = model.evaluate(data_test, labels_test)"]},{"cell_type":"code","execution_count":474,"metadata":{"colab":{"background_save":true},"id":"r31_uxxgmg-G"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss: 127.79%\n","accuracy: 65.90%\n"]}],"source":["print(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1rFZZf9ECknkLNDv8so_kQNPhqain0RqJ","timestamp":1653989613942}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
